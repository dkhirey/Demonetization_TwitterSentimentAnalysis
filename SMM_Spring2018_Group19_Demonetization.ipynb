{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment analysis of Demonetization in India - Group 19\n",
    "\n",
    "This script is developed by-\n",
    "Deepak Khirey\n",
    "Himanshu Gupta\n",
    "Aravind mannarswamy\n",
    "\n",
    "Version Control -\n",
    "V1.0 April 23 2018\n",
    "\n",
    "Description -\n",
    "1. This script takes Twitter Corpus of Demonetization tweets in pickle format as input.\n",
    "2. Performs Data Cleaning operations\n",
    "3. Performs cross validation accuracy comparison of classifiers\n",
    "4. Performs Sentiment Polarity annotation by Supervised Machine Learning method on Tweet corpus Dataframe.\n",
    "5. Performs Vizualization in plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "This is import block. It makes sure we have imported all package before we execute the script.\n",
    "It uses in-built python packages like pandas, numpy, scikitlearn.\n",
    "We have also used some external packages for this project which need to be installed explicitly.\n",
    "\n",
    "1. guess_language\n",
    " https://pypi.org/project/guess-language/\n",
    " pip install guess-language\n",
    "\n",
    "2. tweet_preprocessor\n",
    "https://pypi.org/project/tweet-preprocessor/\n",
    "pip install tweet-preprocessor \n",
    "\n",
    "3. poltly\n",
    "pip install plotly\n",
    "\n",
    "In case if you face error while installing, please refer https://github.com/s/preprocessor/issues/16\n",
    "Fixed zip tweet-preprocessor-0.4.0.zip is included alongwith submission.\n",
    "It can be installed using \n",
    "pip install < path to compressed file>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import os\n",
    "import preprocessor as p\n",
    "import re\n",
    "from guess_language import guess_language\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "plotly.tools.set_credentials_file(username='iuproject', api_key='m3AGEhqrwClGaYCf7zqV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Corpus\n",
    "Reading corpus under directory structured. \n",
    "Corpus is stored in pickle files. \n",
    "After reading all pickle files, data is imported into a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1605240, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read pickle files and append them to a data frame\n",
    "tweets = [] #empty dictionary\n",
    "for root, subdirs, files in os.walk(\".\"):\n",
    "    list_file_path = os.path.join(root, 'here.txt')\n",
    "    with open(list_file_path, 'wb') as list_file:\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            if filename.endswith(\".pkl\"):\n",
    "                tweets.extend(pkl.load(open(file_path, \"rb\"))) #appending pickle file content to this dictionary\n",
    "\n",
    "\n",
    "tweet_df = pd.DataFrame.from_dict(tweets) # dictionary to dataframe\n",
    "\n",
    "tweet_df.to_csv(\"demonetizationtweetsraw.csv\", sep='|', encoding='utf-8') # saving dataframe as csv file\n",
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Data Cleaning step. It removes all un-necessary text from tweets and provides a clean corpus. This may take several minutes. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389312, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedup_tweet_df = tweet_df.drop_duplicates(subset=['tweet_id']) # removing duplicates based on tweet_id\n",
    "dedup_tweet_df = dedup_tweet_df[dedup_tweet_df['text'].str.startswith('RT') == False] # removing Retweets\n",
    "\n",
    "dedup_tweet_df = dedup_tweet_df.replace({'# ': '#'}, regex=True) # removing space after #\n",
    "dedup_tweet_df = dedup_tweet_df.replace({'@ ': '@'}, regex=True)# removing space after @\n",
    "dedup_tweet_df['clean_text'] = dedup_tweet_df['text'].apply(p.clean)\n",
    "\n",
    "cleantext = []\n",
    "for text in dedup_tweet_df['clean_text']:\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text) # keeping only english letters\n",
    "    cleantext.append(text)\n",
    "dedup_tweet_df['clean_text'] = cleantext\n",
    "\n",
    "dedup_tweet_df['clean_text'] = dedup_tweet_df['clean_text'].str.replace('\\W+', ' ')\n",
    "dedup_tweet_df['clean_text'] = dedup_tweet_df['clean_text'].str.lower() # converting all to lowercase\n",
    "\n",
    "\n",
    "lng = []\n",
    "for text in dedup_tweet_df['clean_text']:\n",
    "    lng.append(guess_language(text)) # using guess language\n",
    "dedup_tweet_df['lng'] = lng\n",
    "dedup_tweet_df = dedup_tweet_df[(dedup_tweet_df['lng']==\"en\")==True] # keeping only english language tweets\n",
    "\n",
    "dedup_tweet_df = dedup_tweet_df[(dedup_tweet_df['clean_text'].str.len() <= 10) == False] #removing null and short strings\n",
    "\n",
    "dedup_tweet_df = dedup_tweet_df[['date','favorites','retweets','term','clean_text','tweet_id']] # keeping only required columns\n",
    "dedup_tweet_df.to_csv(\"demonetizationtweetsclean.csv\", sep='|', encoding='utf-8') # saving dataframe as csv file\n",
    "dedup_tweet_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "Reading training datset which is manualy annotated for sentiment polarity. Structure of datset is same as that of sentiment140. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tid</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>polarity</td>\n",
       "      <td>tid</td>\n",
       "      <td>date</td>\n",
       "      <td>query</td>\n",
       "      <td>user</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>796082258591432000</td>\n",
       "      <td>2016-11-08 15:09:35</td>\n",
       "      <td>No_QUERY</td>\n",
       "      <td>GROUP19</td>\n",
       "      <td># ModiJi doesn't give a DAMN about U, Ur RTI a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>796071624894218000</td>\n",
       "      <td>2016-11-08 14:27:20</td>\n",
       "      <td>No_QUERY</td>\n",
       "      <td>GROUP19</td>\n",
       "      <td>great Step of Closure of 500 and 1000 rupee no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>796047865646678000</td>\n",
       "      <td>2016-11-08 12:52:55</td>\n",
       "      <td>No_QUERY</td>\n",
       "      <td>GROUP19</td>\n",
       "      <td># Demonetisation of Rs 500, Rs 1000 notes: DO ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>796031200129351000</td>\n",
       "      <td>2016-11-08 11:46:42</td>\n",
       "      <td>No_QUERY</td>\n",
       "      <td>GROUP19</td>\n",
       "      <td>Will this # Demonetisation of Rs 500 &amp; 1000 no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                 tid                 date     query     user  \\\n",
       "0  polarity                 tid                 date     query     user   \n",
       "1         4  796082258591432000  2016-11-08 15:09:35  No_QUERY  GROUP19   \n",
       "2         4  796071624894218000  2016-11-08 14:27:20  No_QUERY  GROUP19   \n",
       "3         2  796047865646678000  2016-11-08 12:52:55  No_QUERY  GROUP19   \n",
       "4         2  796031200129351000  2016-11-08 11:46:42  No_QUERY  GROUP19   \n",
       "\n",
       "                                                text  \n",
       "0                                               text  \n",
       "1  # ModiJi doesn't give a DAMN about U, Ur RTI a...  \n",
       "2  great Step of Closure of 500 and 1000 rupee no...  \n",
       "3  # Demonetisation of Rs 500, Rs 1000 notes: DO ...  \n",
       "4  Will this # Demonetisation of Rs 500 & 1000 no...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynewtrainingclean = pd.read_csv(\"demonetizationtrainingdata.csv\", names=[\"polarity\", \"tid\", \"date\", \"query\", \"user\", \"text\"], encoding=\"latin1\")\n",
    "mynewtrainingclean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing Support Vector Machine classifier on training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hv158\\AppData\\Local\\Continuum\\anaconda36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning:\n",
      "\n",
      "The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][0.82176092 0.89541547 0.91117479 0.91176471 0.89096126]\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer(min_df=10,lowercase=True,stop_words='english')\n",
    "X = tv.fit_transform(mynewtrainingclean['text'])\n",
    "y = mynewtrainingclean['polarity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3057)\n",
    "clf = SVC(kernel=\"linear\", verbose=3)\n",
    "clf.fit(X_train, y_train)\n",
    "cv_scores = cross_val_score(clf, X, y,cv=5)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing Logistic Regression classifier on training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hv158\\AppData\\Local\\Continuum\\anaconda36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning:\n",
      "\n",
      "The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84395132 0.89326648 0.89971347 0.89885222 0.88880918]\n"
     ]
    }
   ],
   "source": [
    "tv1 = TfidfVectorizer(min_df=10,lowercase=True,stop_words='english')\n",
    "X1 = tv1.fit_transform(mynewtrainingclean['text'])\n",
    "y1 = mynewtrainingclean['polarity']\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.25, random_state=3057)\n",
    "clf1 = LogisticRegression()\n",
    "clf1.fit(X1_train, y1_train)\n",
    "cv_scores = cross_val_score(clf1, X1, y1,cv=5)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing K Nearest Neighbor classifier on training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hv158\\AppData\\Local\\Continuum\\anaconda36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning:\n",
      "\n",
      "The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77308518 0.79297994 0.82234957 0.8507891  0.80703013]\n"
     ]
    }
   ],
   "source": [
    "tv2 = TfidfVectorizer(min_df=10,lowercase=True,stop_words='english')\n",
    "X2 = tv2.fit_transform(mynewtrainingclean['text'])\n",
    "y2 = mynewtrainingclean['polarity']\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.25, random_state=3057)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=15)\n",
    "clf2.fit(X2_train, y2_train)\n",
    "cv_scores = cross_val_score(clf2, X2, y2,cv=5)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Support Vector Machine classifier performed better in terms of accuracy, using this for predicting sentiments on entire corpus. This may take several minutes. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tfidf = tv.transform(dedup_tweet_df['clean_text'])\n",
    "tweet_pred = clf.predict(tweet_tfidf)\n",
    "dedup_tweet_df[\"sent_pred\"] = tweet_pred\n",
    "dedup_tweet_df.to_csv(\"demonetizationtweetswithscore.csv\", sep='|', encoding='utf-8') # saving dataframe as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing final results of sentiment polarity in CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>term</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sent_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-11-08 18:58:13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#Demonetisation</td>\n",
       "      <td>why are opposing does nodemonetization helps f...</td>\n",
       "      <td>796139793998450688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-08 18:57:24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#Demonetisation</td>\n",
       "      <td>of rs rs notes naidu had inkling of the ban</td>\n",
       "      <td>796139591216599042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-11-08 18:38:58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#Demonetisation</td>\n",
       "      <td>he said he should have bought gold on dhan tir...</td>\n",
       "      <td>796134950005342211</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-11-08 18:10:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#Demonetisation</td>\n",
       "      <td>illiterate lots are always forced to delete th...</td>\n",
       "      <td>796127842614579200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-11-08 18:06:38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#Demonetisation</td>\n",
       "      <td>richbm people in next hrs rushing to hospitals...</td>\n",
       "      <td>796126814745731072</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  favorites  retweets             term  \\\n",
       "0 2016-11-08 18:58:13          1         0  #Demonetisation   \n",
       "1 2016-11-08 18:57:24          0         0  #Demonetisation   \n",
       "2 2016-11-08 18:38:58          0         0  #Demonetisation   \n",
       "3 2016-11-08 18:10:43          0         0  #Demonetisation   \n",
       "4 2016-11-08 18:06:38          0         0  #Demonetisation   \n",
       "\n",
       "                                          clean_text            tweet_id  \\\n",
       "0  why are opposing does nodemonetization helps f...  796139793998450688   \n",
       "1        of rs rs notes naidu had inkling of the ban  796139591216599042   \n",
       "2  he said he should have bought gold on dhan tir...  796134950005342211   \n",
       "3  illiterate lots are always forced to delete th...  796127842614579200   \n",
       "4  richbm people in next hrs rushing to hospitals...  796126814745731072   \n",
       "\n",
       "  sent_pred  \n",
       "0         0  \n",
       "1         0  \n",
       "2         4  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_results = pd.read_csv(\"demonetizationtweetswithscore.csv\", sep='|', encoding='utf-8') # saving dataframe as csv file\n",
    "dedup_tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "We are using plotly package for our visualizations. We have registered to plotly account which provides 25 graphs for free. We have noticed that sometimes plotly doesn't work in secured environemnt through firewalls, or graph limit is reached if executed multiple times. \n",
    "Hence we have also created these visualizations in Tableau Public. It can be found at below link -\n",
    "https://public.tableau.com/views/SMMFinal/Sheet1?:embed=y&:display_count=yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization 1- Twitter Trends by Hashtag\n",
    "Using plotly for getting Twitter volume over timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~iuproject/99.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_results['date_notime'] = pd.to_datetime(stored_results['date']).dt.date\n",
    "data = stored_results.groupby(['date_notime','term']).size().reset_index(name='count')\n",
    "data1 = data[data['term'] == '#DeMonetisation']\n",
    "data2 = data[data['term'] == '#Demonetisation']\n",
    "data3 = data[data['term'] == '#demonetisation']\n",
    "data4 = data[data['term'] == '#demonetization']\n",
    "data5 = data[data['term'] == '#Demonetization']\n",
    "data6 = data[data['term'] == '#DeMonetization']\n",
    "DeMonetisation = go.Scatter(x=data1['date_notime'], y=data1['count'], mode = 'lines', name = '#DeMonetisation')\n",
    "Demonetisation = go.Scatter(x=data2['date_notime'], y=data2['count'], mode = 'lines', name = '#Demonetisation')\n",
    "demonetisation = go.Scatter(x=data3['date_notime'], y=data3['count'], mode = 'lines', name = '#demonetisation')\n",
    "demonetization = go.Scatter(x=data4['date_notime'], y=data4['count'], mode = 'lines', name = '#demonetization')\n",
    "Demonetization = go.Scatter(x=data5['date_notime'], y=data5['count'], mode = 'lines', name = '#Demonetization')\n",
    "DeMonetization = go.Scatter(x=data6['date_notime'], y=data6['count'], mode = 'lines', name = '#DeMonetization')\n",
    "line = [DeMonetisation, Demonetisation, demonetisation, demonetization, Demonetization, DeMonetization]\n",
    "layout = dict(title=\"Tweet count by date & hashtags\",yaxis=dict(title='No of tweets') )\n",
    "fig = dict(data=line, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization 2 -\n",
    "Twitter sentiment polarity over timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~iuproject/93.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_results['date_notime'] = pd.to_datetime(stored_results['date']).dt.date\n",
    "data = stored_results.groupby(['date_notime','sent_pred']).size().reset_index(name='count')\n",
    "data1 = data[data['sent_pred'] == 0]\n",
    "data2 = data[data['sent_pred'] == 2]\n",
    "data3 = data[data['sent_pred'] == 4]\n",
    "Negative = go.Scatter(x=data1['date_notime'], y=data1['count'], mode = 'lines', name = 'Negative')\n",
    "Neutral = go.Scatter(x=data2['date_notime'], y=data2['count'], mode = 'lines', name = 'Neutral')\n",
    "Positive = go.Scatter(x=data3['date_notime'], y=data3['count'], mode = 'lines', name = 'Positive')\n",
    "line = [Neutral, Positive, Negative]\n",
    "layout = dict(title=\"Tweet count by date & predicted sentiment\",yaxis=dict(title='No of tweets') )\n",
    "fig = dict(data=line, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization 3 -\n",
    "Favorites by Sentiment polarity over timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~iuproject/95.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_results['date_notime'] = pd.to_datetime(stored_results['date']).dt.date\n",
    "data = stored_results.groupby(['date_notime','sent_pred'])['favorites'].agg('sum').reset_index(name='sum')\n",
    "data1 = data[data['sent_pred'] == 0]\n",
    "data2 = data[data['sent_pred'] == 2]\n",
    "data3 = data[data['sent_pred'] == 4]\n",
    "Negative = go.Scatter(x=data1['date_notime'], y=data1['sum'], mode = 'lines', name = 'Negative')\n",
    "Neutral = go.Scatter(x=data2['date_notime'], y=data2['sum'], mode = 'lines', name = 'Neutral')\n",
    "Positive = go.Scatter(x=data3['date_notime'], y=data3['sum'], mode = 'lines', name = 'Positive')\n",
    "line = [Neutral, Positive, Negative]\n",
    "layout = dict(title=\"Favorites by date & predicted sentiment\",yaxis=dict(title='No of Favorites tweets') )\n",
    "fig = dict(data=line, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization 4 -\n",
    "Retweets by Sentiment polarity over timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~iuproject/97.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = stored_results.groupby(['date_notime','sent_pred'])['retweets'].agg('sum').reset_index(name='sum')\n",
    "data1 = data[data['sent_pred'] == 0]\n",
    "data2 = data[data['sent_pred'] == 2]\n",
    "data3 = data[data['sent_pred'] == 4]\n",
    "Negative = go.Scatter(x=data1['date_notime'], y=data1['sum'], mode = 'lines', name = 'Negative')\n",
    "Neutral = go.Scatter(x=data2['date_notime'], y=data2['sum'], mode = 'lines', name = 'Neutral')\n",
    "Positive = go.Scatter(x=data3['date_notime'], y=data3['sum'], mode = 'lines', name = 'Positive')\n",
    "line = [Neutral, Positive, Negative]\n",
    "layout = dict(title=\"Retweets by date & predicted sentiment\",yaxis=dict(title='No of retweetes') )\n",
    "fig = dict(data=line, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
